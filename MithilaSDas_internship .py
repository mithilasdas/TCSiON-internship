# -*- coding: utf-8 -*-
"""MithilaSDas_internship.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RKutH8U9QLIgdez5JqHphGjVUArKkouO

# **1. Load the dataset**
"""

#IMPORT LIBRARIES
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#import missingno as msno
import warnings
warnings.filterwarnings('ignore')

#IMPORT LOAD DATA
data=pd.read_csv('salarydata.csv')

#IMPORT READ DATA
data.head()

data.tail()

#CHECK DATA INFORMATION
data.info()

#DESCRIBE DATA
data.describe()

#CHECK SHAPE OF DATA
data.shape

#CHECK DATA COLUMNS
data.columns

#CHECK UNIQUE VALUES
data.nunique()

"""# **2. Exploratory Data Analysis (EDA)**"""

data.hist(figsize=(20,15))
plt.show()

plt.hist(data['age'])
plt.xlabel('Age')
plt.ylabel('Count')
plt.xticks(np.arange(20,100,5))
plt.rcParams['figure.figsize'] = (10,10)
plt.title('Age Distribution')
plt.show()

data['education'].value_counts().plot(kind = 'bar')
plt.xlabel('Education')
plt.ylabel('Count')
plt.rcParams['figure.figsize'] = (10,10)
plt.title('Education level')
plt.show()

sns.countplot(x=data['sex'])
plt.title('gender count')

data['marital-status'].value_counts().plot(kind = 'pie',subplots=True, autopct='%1.1f%%')
plt.rcParams['figure.figsize'] = (10,10)
plt.title('marital status')
plt.show()

data['relationship'].value_counts().plot(kind = 'pie',subplots=True, autopct='%1.1f%%')
plt.rcParams['figure.figsize'] = (10,10)
plt.title('Relationship')
plt.show()

sns.countplot(x=data['occupation'])
plt.xticks(rotation=90)
plt.title('occupation status')
plt.show

"""# **2.Data Preprocessing**

**2.1.HANDLING MISSING VALUES**
"""

#check null values
data.isna().sum()

#CHECK DATA TYPES
data.dtypes

"""*REPLACED MISSING VALUES WITH MEAN*"""

for i in ['capital-gain','capital-loss', 'hours-per-week']:
      data[i]=data[i].fillna(data[i].mean())

"""*REPLACED MISSING VALUES(CATEGORICAL) WITH MODE*"""

for i in ['sex','native-country', 'salary']:
      data[i]=data[i].fillna(data[i].mode()[0])

#RECHECK NULL VALUES
data.isna().sum()

"""# **2.2 CORRELATION**"""

corrmatrix=data.corr()
plt.subplots(figsize=(10,4))
sns.heatmap(corrmatrix,vmin=0.2,vmax=0.9,annot=True,cmap='Blues')

"""DROP UNNECESSARY COLUMNS"""

data.drop(['capital-gain','capital-loss','education-num'], axis = 1,inplace = True)

"""# **2.3 REMOVING OR MODIFYING OUTLIERS**

*OUTLIER DETECTION*
"""

num_col = data.select_dtypes(include=np.number).columns.tolist()
plt.figure(figsize=(20,30))

for i, variable in enumerate(num_col):
                     plt.subplot(5,5,i+1)
                     plt.boxplot(data[variable],whis=1.5)
                     plt.tight_layout()
                     plt.title(variable)

"""*MODIFYING OUTLIERS*"""

for i in num_col:
    Q1=data[i].quantile(0.25) # 25th quantile
    Q3=data[i].quantile(0.75)  # 75th quantile
    IQR=Q3-Q1
    Lower_Whisker = Q1 - 1.5*IQR 
    Upper_Whisker = Q3 + 1.5*IQR
    data[i] = np.clip(data[i], Lower_Whisker, Upper_Whisker)

plt.figure(figsize=(20,30))

for i, variable in enumerate(num_col):
                     plt.subplot(5,5,i+1)
                     plt.boxplot(data[variable],whis=1.5)
                     plt.tight_layout()
                     plt.title(variable)

from sklearn import preprocessing 
label= preprocessing.LabelEncoder()  
data['workclass']=label.fit_transform(data['workclass'])
data['education']=label.fit_transform(data['education'])
data['occupation']=label.fit_transform(data['occupation'])
data['sex']=label.fit_transform(data['sex'])
#data['salary']=label.fit_transform(data['salary'])
data['race']=label.fit_transform(data['race'])
data['native-country']=label.fit_transform(data['native-country'])
data['marital-status']=label.fit_transform(data['marital-status'])
data['relationship']=label.fit_transform(data['relationship'])

data

"""# **Standard Scaling**"""

X=data.drop(columns=['salary'],axis=1)
y=data['salary']

#X.describe()

#X.columns

#import library
#from sklearn.preprocessing import StandardScaler

#fitting the model
#scaler=StandardScaler()
#X=scaler.fit_transform(X)

#type(X)

#X=pd.DataFrame(X,columns=['age', 'workclass', 'education', 'marital-status', 'occupation','relationship', 'race', 'sex', 'hours-per-week', 'native-country'])

#X

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

"""# **Logistic regression**"""

#import logistic regression
from sklearn.linear_model import LogisticRegression
lor=LogisticRegression()
lor_model=lor.fit(X_train,y_train)
y_predict_lor=lor_model.predict(X_test)

y_predict_lor

"""*confusion matrix*"""

#import confusion matrix
from sklearn.metrics import confusion_matrix

#confusion matrix
confusion_matrix(y_test,y_predict_lor)

"""*precision,accuracy and recall score*"""

#import precision score and recall score
from sklearn.metrics import precision_score,recall_score

#precision score
precision_score(y_test,y_predict_lor,pos_label=1,average='micro')

#recallscore
recall_score(y_test,y_predict_lor,pos_label=1,average='micro')

#check accuracy
from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_predict_lor)

R1=accuracy_score(y_test,y_predict_lor)*100
R1

"""# **K-Nearest Neighbor(KNN) classifier**"""

#import KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier

metric_k=[]
neighbors=range(3,20)
for k in neighbors:
  classifier= KNeighborsClassifier(n_neighbors=k)
  classifier=classifier.fit(X_train,y_train)
  y_pred_kNN=classifier.predict(X_test)
  acc=accuracy_score(y_test,y_pred_kNN)
  metric_k.append(acc)

#print metric
metric_k

plt.plot(neighbors,metric_k,'o-')
plt.xlabel('k values')
plt.ylabel('Accuracies')
plt.grid()

classifier= KNeighborsClassifier(n_neighbors=5)
classifier=classifier.fit(X_train,y_train)
y_pred_kNN=classifier.predict(X_test)

#find confusion matrix
confusion_matrix(y_test,y_pred_kNN)

#find precision score
precision_score(y_test,y_pred_kNN,pos_label=1,average='micro')

#find recall score
recall_score(y_test,y_pred_kNN,pos_label=1,average='micro')

#find accuracy score
accuracy_score(y_test,y_pred_kNN)

R2=accuracy_score(y_test,y_pred_kNN)*100

"""# **Support Vector Machine (SVM) Classifier**"""

#import Support Vector Classification
from sklearn.svm import SVC

"""*kernel type- linear*"""

svm_clf=SVC(kernel='linear')
svm_clf=svm_clf.fit(X_train,y_train)
y_pred_svm=svm_clf.predict(X_test)

#find confusion_matrix
confusion_matrix(y_test,y_pred_svm)

#find precision score
precision_score(y_test,y_pred_svm,pos_label=1,average='micro')

#find recall score
recall_score(y_test,y_pred_svm,pos_label=1,average='micro')

#find accuracy score
accuracy_score(y_test,y_pred_svm)

R3=accuracy_score(y_test,y_pred_svm)*100

"""*kernel type-RBF*"""

svm_clf2=SVC(kernel='rbf')
svm_clf2=svm_clf2.fit(X_train,y_train)
y_pred_svm2=svm_clf2.predict(X_test)

#find confusion_matrix
confusion_matrix(y_test,y_pred_svm2)

#find precision score
precision_score(y_test,y_pred_svm2,pos_label=1,average='micro')

#find recall score
recall_score(y_test,y_pred_svm2,pos_label=1,average='micro')

#find accuracy score
accuracy_score(y_test,y_pred_svm2)

R4=accuracy_score(y_test,y_pred_svm2)*100

"""# **Decision tree classifier**"""

#import decision tree
from sklearn.tree import DecisionTreeClassifier

dt_clf= DecisionTreeClassifier()
dt_clf=dt_clf.fit(X_train,y_train)
y_pred_dt=dt_clf.predict(X_test)

#find confusion matrix
confusion_matrix(y_test,y_pred_dt)

#find precision score
precision_score(y_test,y_pred_dt,pos_label=1,average='micro')

#find recall score
recall_score(y_test,y_pred_dt,pos_label=1,average='micro')

#find accuracy score
accuracy_score(y_test,y_pred_dt)

R5=accuracy_score(y_test,y_pred_dt)*100

"""# **Random forest classifier**"""

#import random forest classifier
from sklearn.ensemble import RandomForestClassifier

rf_clf=RandomForestClassifier()
rf_clf=rf_clf.fit(X_train,y_train)
y_pred_rf=rf_clf.predict(X_test)

#find confusion matrix
confusion_matrix(y_test,y_pred_rf)

#find precision score
precision_score(y_test,y_pred_rf,pos_label=1,average='micro')

#find recall score
recall_score(y_test,y_pred_rf,pos_label=1,average='micro')

#find accuracy score
accuracy_score(y_test,y_pred_rf)

R6=accuracy_score(y_test,y_pred_rf)*100

"""# **AdaBoost Classifier**"""

from sklearn.ensemble import AdaBoostClassifier
clf = AdaBoostClassifier(n_estimators=200, random_state=0)

clf.fit(X_train, y_train)

pred=clf.predict(X_test)

#find confusion matrix
confusion_matrix(y_test,pred)

#find precision score
precision_score(y_test,pred,pos_label=1,average='micro')

#find recall score
recall_score(y_test,pred,pos_label=1,average='micro')

#find accuracy score
accuracy_score(y_test,pred)

R7=accuracy_score(y_test,pred)*100

"""# **Model results**"""

metric_results= {'Model': ['Logistic Regression','KNeighbors', 'SVM(linear)','SVM(RBF)','Decision Tree','Random Forest','AdaBoost'], 
                 'Accuracy': [R1, R2, R3,R4,R5,R6,R7]}
metrics= pd.DataFrame(metric_results)
metrics

"""***AdaBoost classfier shows better accuracy.***"""

# save the model
import pickle
filename = 'model.pkl'
pickle.dump(clf, open(filename, 'wb'))